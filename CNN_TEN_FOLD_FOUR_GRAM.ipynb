{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Loading necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "from gensim.models.wrappers import FastText\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import models\n",
    "from sklearn.model_selection import KFold # import KFold\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Input, Dense, concatenate, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, LSTM, Conv1D, Flatten, MaxPooling1D\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score, accuracy_score\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Loading pretrained word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_cbow = FastText.load_fasttext_format('../cc.ne.300.bin/cc.ne.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 584436 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "for w in model_ug_cbow.wv.vocab.keys():\n",
    "    embeddings_index[w] = model_ug_cbow.wv[w]#np.append(model_ug_cbow.wv[w],model_ug_sg.wv[w])#\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Loading Trained word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_sg = models.Word2Vec.load('../w2v_model_ug_sg.word2vec')\n",
    "model_ug_cbow = models.Word2Vec.load('../w2v_model_ug_cbow.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 42409 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "for w in model_ug_cbow.wv.vocab.keys():\n",
    "    embeddings_index[w] = model_ug_cbow.wv[w]#\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"NepaliEarthquakeTweets_plus_blockade.xlsx\",sheetname=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_indexes = np.where(data['bhawna']=='p')\n",
    "neg_indexes = np.where(data['bhawna']=='n')\n",
    "total_size = 600\n",
    "size = 600\n",
    "val_size = 600\n",
    "df_pos = pd.DataFrame.from_items([('text',data['tweet_text'][pos_indexes[0][0:total_size]]),('target',0)])\n",
    "df_neg = pd.DataFrame.from_items([('text',data['tweet_text'][neg_indexes[0][0:total_size]]),('target',1)])\n",
    "\n",
    "df_pos['reply'] = data['reply'][pos_indexes[0][0:total_size]]\n",
    "df_pos['retweet'] = data['retweet'][pos_indexes[0][0:total_size]]\n",
    "df_pos['likes'] = data['likes'][pos_indexes[0][0:total_size]]\n",
    "\n",
    "df_neg['reply'] = data['reply'][neg_indexes[0][0:total_size]]\n",
    "df_neg['retweet'] = data['retweet'][neg_indexes[0][0:total_size]]\n",
    "df_neg['likes'] = data['likes'][neg_indexes[0][0:total_size]]\n",
    "\n",
    "df_pos.index = range(len(df_pos.index))\n",
    "df_neg.index = range(len(df_neg.index))\n",
    "\n",
    "df_pos_f = df_pos\n",
    "df_neg_f = df_neg\n",
    "df_F = df_pos[0:size]\n",
    "df_F = df_F.append(df_neg[0:size])\n",
    "df_F.index = range(len(df_F.index))\n",
    "\n",
    "len(df_F)\n",
    "validation_df_F= df_pos[500:600]\n",
    "validation_df_F = validation_df_F.append(df_neg[500:600])\n",
    "validation_df_F.index = range(len(validation_df_F.index))\n",
    "#validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_pos[0:size]\n",
    "df = df.append(df_neg[0:size])\n",
    "df.index = range(len(df.index))\n",
    "validation_df= df_pos[500:600]\n",
    "validation_df = validation_df.append(df_neg[500:600])\n",
    "validation_df.index = range(len(validation_df.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Data Preprocessing: Removing puncuation, english text, decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "pat3 = r'([@#][A-Za-z0-9]+)'\n",
    "pat4 = r'.[A-Za-z0-9./]+'\n",
    "pat5 = r'[\\,۔،۔”–’‘‘_!…।-]|(\")|(:)|(%)|(ः)|(\\u200d)|(\\xa0…)|(\\u200c\\u200c)'\n",
    "combined_pat = r'|'.join((pat1, pat2,pat3,pat4,pat5))\n",
    "df['text'] = [re.sub(combined_pat, ' ', x) for x in df['text']]\n",
    "validation_df['text'] = [re.sub(combined_pat, ' ', x) for x in validation_df['text']]\n",
    "df_pos['text'] = [re.sub(combined_pat, ' ', x) for x in df_pos['text']]\n",
    "df_neg['text'] = [re.sub(combined_pat, ' ', x) for x in df_neg['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=10, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=10)\n",
    "print(kf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Define Necessary funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(df_pos,df_neg,train_index,test_index):\n",
    "    X_train = df_pos.text[train_index.tolist()]\n",
    "    X_train = X_train.append(df_neg.text[train_index.tolist()])\n",
    "    X_train.index = range(len(X_train.index))\n",
    "    \n",
    "    y_train = df_pos.target[train_index.tolist()]\n",
    "    y_train = y_train.append(df_neg.target[train_index.tolist()])\n",
    "    y_train.index = range(len(y_train.index))\n",
    "    \n",
    "    X_test = df_pos.text[test_index.tolist()]\n",
    "    X_test = X_test.append(df_neg.text[test_index.tolist()])\n",
    "    X_test.index = range(len(X_test.index))\n",
    "    \n",
    "    y_test = df_pos.target[test_index.tolist()]\n",
    "    y_test = y_test.append(df_neg.target[test_index.tolist()])\n",
    "    y_test.index = range(len(y_test.index))\n",
    "    \n",
    "    return(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.model.validation_data[0]))).round()\n",
    "        val_targ = self.model.validation_data[1]\n",
    "        _val_f1 = f1_score(val_targ, val_predict)\n",
    "        _val_recall = recall_score(val_targ, val_predict)\n",
    "        _val_precision = precision_score(val_targ, val_predict)\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        #print(\" — val_f1: %f — val_precision: %f — val_recall %f\" %(_val_f1, _val_precision, _val_recall))\n",
    "        return\n",
    "\n",
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Constructing CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "\n",
      "Fold 1\n",
      "====================================\n",
      "\n",
      "Train on 1080 samples, validate on 200 samples\n",
      "Epoch 1/5\n",
      "1080/1080 [==============================] - 19s 17ms/step - loss: 0.7975 - accuracy: 0.5333 - val_loss: 0.6367 - val_accuracy: 0.6850\n",
      "Epoch 2/5\n",
      "1080/1080 [==============================] - 18s 17ms/step - loss: 0.5774 - accuracy: 0.6907 - val_loss: 0.4358 - val_accuracy: 0.8200\n",
      "Epoch 3/5\n",
      "1080/1080 [==============================] - 17s 16ms/step - loss: 0.4587 - accuracy: 0.7694 - val_loss: 0.3298 - val_accuracy: 0.9200\n",
      "Epoch 4/5\n",
      "1080/1080 [==============================] - 18s 16ms/step - loss: 0.3418 - accuracy: 0.8454 - val_loss: 0.1851 - val_accuracy: 0.9700\n",
      "Epoch 5/5\n",
      "1080/1080 [==============================] - 17s 16ms/step - loss: 0.2431 - accuracy: 0.8963 - val_loss: 0.1170 - val_accuracy: 0.9700\n",
      "Validation Confusion Matrix\n",
      "[[99  1]\n",
      " [ 5 95]]\n",
      "Test Confusion Matrix\n",
      "[[45 15]\n",
      " [23 37]]\n",
      "=========================================\n",
      "\n",
      "Fold 2\n",
      "====================================\n",
      "\n",
      "Train on 1080 samples, validate on 200 samples\n",
      "Epoch 1/5\n",
      "1080/1080 [==============================] - 18s 17ms/step - loss: 0.7838 - accuracy: 0.5759 - val_loss: 0.5303 - val_accuracy: 0.8150\n",
      "Epoch 2/5\n",
      "1080/1080 [==============================] - 17s 16ms/step - loss: 0.5343 - accuracy: 0.7241 - val_loss: 0.4051 - val_accuracy: 0.8650\n",
      "Epoch 3/5\n",
      "1080/1080 [==============================] - 20s 18ms/step - loss: 0.4148 - accuracy: 0.8000 - val_loss: 0.2640 - val_accuracy: 0.9150\n",
      "Epoch 4/5\n",
      "1080/1080 [==============================] - 19s 17ms/step - loss: 0.3200 - accuracy: 0.8602 - val_loss: 0.1771 - val_accuracy: 0.9700\n",
      "Epoch 5/5\n",
      "1080/1080 [==============================] - 20s 18ms/step - loss: 0.2387 - accuracy: 0.9102 - val_loss: 0.1064 - val_accuracy: 0.9850\n",
      "Validation Confusion Matrix\n",
      "[[99  1]\n",
      " [ 2 98]]\n",
      "Test Confusion Matrix\n",
      "[[35 25]\n",
      " [24 36]]\n",
      "=========================================\n",
      "\n",
      "Fold 3\n",
      "====================================\n",
      "\n",
      "Train on 1080 samples, validate on 200 samples\n",
      "Epoch 1/5\n",
      "1080/1080 [==============================] - 20s 18ms/step - loss: 0.8161 - accuracy: 0.5176 - val_loss: 0.6272 - val_accuracy: 0.7400\n",
      "Epoch 2/5\n",
      "1080/1080 [==============================] - 17s 16ms/step - loss: 0.5843 - accuracy: 0.6815 - val_loss: 0.5091 - val_accuracy: 0.7700\n",
      "Epoch 3/5\n",
      "1080/1080 [==============================] - 17s 16ms/step - loss: 0.4819 - accuracy: 0.7639 - val_loss: 0.3804 - val_accuracy: 0.8900\n",
      "Epoch 4/5\n",
      "1080/1080 [==============================] - 19s 17ms/step - loss: 0.3563 - accuracy: 0.8278 - val_loss: 0.2564 - val_accuracy: 0.9000\n",
      "Epoch 5/5\n",
      "1080/1080 [==============================] - 18s 16ms/step - loss: 0.2416 - accuracy: 0.9102 - val_loss: 0.1155 - val_accuracy: 0.9850\n",
      "Validation Confusion Matrix\n",
      "[[ 97   3]\n",
      " [  0 100]]\n",
      "Test Confusion Matrix\n",
      "[[46 14]\n",
      " [ 8 52]]\n",
      "=========================================\n",
      "\n",
      "Fold 4\n",
      "====================================\n",
      "\n",
      "Train on 1080 samples, validate on 200 samples\n",
      "Epoch 1/5\n",
      "1080/1080 [==============================] - 18s 17ms/step - loss: 0.8394 - accuracy: 0.5361 - val_loss: 0.6368 - val_accuracy: 0.6600\n",
      "Epoch 2/5\n",
      "1080/1080 [==============================] - 17s 16ms/step - loss: 0.5774 - accuracy: 0.6796 - val_loss: 0.4917 - val_accuracy: 0.8050\n",
      "Epoch 3/5\n",
      "1080/1080 [==============================] - 19s 17ms/step - loss: 0.4723 - accuracy: 0.7630 - val_loss: 0.3647 - val_accuracy: 0.9150\n",
      "Epoch 4/5\n",
      "1080/1080 [==============================] - 19s 18ms/step - loss: 0.3415 - accuracy: 0.8481 - val_loss: 0.2320 - val_accuracy: 0.9450\n",
      "Epoch 5/5\n",
      "1080/1080 [==============================] - 21s 19ms/step - loss: 0.2578 - accuracy: 0.8880 - val_loss: 0.1648 - val_accuracy: 0.9750\n",
      "Validation Confusion Matrix\n",
      "[[96  4]\n",
      " [ 1 99]]\n",
      "Test Confusion Matrix\n",
      "[[39 21]\n",
      " [19 41]]\n",
      "=========================================\n",
      "\n",
      "Fold 5\n",
      "====================================\n",
      "\n",
      "Train on 1080 samples, validate on 200 samples\n",
      "Epoch 1/5\n",
      "1080/1080 [==============================] - 25s 23ms/step - loss: 1.1842 - accuracy: 0.5139 - val_loss: 0.6142 - val_accuracy: 0.6350\n",
      "Epoch 2/5\n",
      "1080/1080 [==============================] - 20s 18ms/step - loss: 0.5938 - accuracy: 0.6769 - val_loss: 0.5015 - val_accuracy: 0.7700\n",
      "Epoch 3/5\n",
      "1080/1080 [==============================] - 34s 32ms/step - loss: 0.4876 - accuracy: 0.7278 - val_loss: 0.3657 - val_accuracy: 0.9150\n",
      "Epoch 4/5\n",
      "1080/1080 [==============================] - 31s 28ms/step - loss: 0.4152 - accuracy: 0.8093 - val_loss: 0.2667 - val_accuracy: 0.9400\n",
      "Epoch 5/5\n",
      "1080/1080 [==============================] - 35s 32ms/step - loss: 0.3305 - accuracy: 0.8454 - val_loss: 0.2033 - val_accuracy: 0.9650\n",
      "Validation Confusion Matrix\n",
      "[[96  4]\n",
      " [ 3 97]]\n",
      "Test Confusion Matrix\n",
      "[[42 18]\n",
      " [24 36]]\n",
      "=========================================\n",
      "\n",
      "Fold 6\n",
      "====================================\n",
      "\n",
      "Train on 1080 samples, validate on 200 samples\n",
      "Epoch 1/5\n",
      "1080/1080 [==============================] - 31s 29ms/step - loss: 1.2153 - accuracy: 0.5537 - val_loss: 0.5587 - val_accuracy: 0.7600\n",
      "Epoch 2/5\n",
      "1080/1080 [==============================] - 28s 26ms/step - loss: 0.5611 - accuracy: 0.6972 - val_loss: 0.4212 - val_accuracy: 0.8550\n",
      "Epoch 3/5\n",
      "1080/1080 [==============================] - 28s 26ms/step - loss: 0.4371 - accuracy: 0.7898 - val_loss: 0.2935 - val_accuracy: 0.9350\n",
      "Epoch 4/5\n",
      "1080/1080 [==============================] - 27s 25ms/step - loss: 0.3325 - accuracy: 0.8537 - val_loss: 0.1810 - val_accuracy: 0.9500\n",
      "Epoch 5/5\n",
      "1080/1080 [==============================] - 25s 23ms/step - loss: 0.2275 - accuracy: 0.9167 - val_loss: 0.1174 - val_accuracy: 0.9800\n",
      "Validation Confusion Matrix\n",
      "[[97  3]\n",
      " [ 1 99]]\n",
      "Test Confusion Matrix\n",
      "[[46 14]\n",
      " [14 46]]\n",
      "=========================================\n",
      "\n",
      "Fold 7\n",
      "====================================\n",
      "\n",
      "Train on 1080 samples, validate on 200 samples\n",
      "Epoch 1/5\n",
      "1080/1080 [==============================] - 30s 27ms/step - loss: 0.7819 - accuracy: 0.5481 - val_loss: 0.5940 - val_accuracy: 0.7200\n",
      "Epoch 2/5\n",
      "1080/1080 [==============================] - 32s 30ms/step - loss: 0.5890 - accuracy: 0.6787 - val_loss: 0.4603 - val_accuracy: 0.8250\n",
      "Epoch 3/5\n",
      "1080/1080 [==============================] - 33s 30ms/step - loss: 0.4638 - accuracy: 0.7685 - val_loss: 0.3274 - val_accuracy: 0.9300\n",
      "Epoch 4/5\n",
      "1080/1080 [==============================] - 32s 29ms/step - loss: 0.3742 - accuracy: 0.8213 - val_loss: 0.2337 - val_accuracy: 0.9100\n",
      "Epoch 5/5\n",
      "1080/1080 [==============================] - 31s 28ms/step - loss: 0.2749 - accuracy: 0.8898 - val_loss: 0.1431 - val_accuracy: 0.9800\n",
      "Validation Confusion Matrix\n",
      "[[97  3]\n",
      " [ 1 99]]\n",
      "Test Confusion Matrix\n",
      "[[48 12]\n",
      " [21 39]]\n",
      "=========================================\n",
      "\n",
      "Fold 8\n",
      "====================================\n",
      "\n",
      "Train on 1080 samples, validate on 200 samples\n",
      "Epoch 1/5\n",
      "1080/1080 [==============================] - 40s 37ms/step - loss: 0.9784 - accuracy: 0.5361 - val_loss: 0.6016 - val_accuracy: 0.6900\n",
      "Epoch 2/5\n",
      "1080/1080 [==============================] - 36s 34ms/step - loss: 0.5771 - accuracy: 0.6833 - val_loss: 0.4679 - val_accuracy: 0.7900\n",
      "Epoch 3/5\n",
      "1080/1080 [==============================] - 45s 42ms/step - loss: 0.4443 - accuracy: 0.7926 - val_loss: 0.3148 - val_accuracy: 0.9000\n",
      "Epoch 4/5\n",
      "1080/1080 [==============================] - 67s 62ms/step - loss: 0.3231 - accuracy: 0.8537 - val_loss: 0.2078 - val_accuracy: 0.9600\n",
      "Epoch 5/5\n",
      "1080/1080 [==============================] - 52s 48ms/step - loss: 0.2498 - accuracy: 0.9074 - val_loss: 0.1156 - val_accuracy: 0.9950\n",
      "Validation Confusion Matrix\n",
      "[[ 99   1]\n",
      " [  0 100]]\n",
      "Test Confusion Matrix\n",
      "[[41 19]\n",
      " [27 33]]\n",
      "=========================================\n",
      "\n",
      "Fold 9\n",
      "====================================\n",
      "\n",
      "Train on 1080 samples, validate on 200 samples\n",
      "Epoch 1/5\n",
      "1080/1080 [==============================] - 36s 34ms/step - loss: 1.2539 - accuracy: 0.5870 - val_loss: 0.5603 - val_accuracy: 0.7350\n",
      "Epoch 2/5\n",
      "1080/1080 [==============================] - 43s 40ms/step - loss: 0.5043 - accuracy: 0.7481 - val_loss: 0.4393 - val_accuracy: 0.8600\n",
      "Epoch 3/5\n",
      "1080/1080 [==============================] - 40s 37ms/step - loss: 0.3732 - accuracy: 0.8444 - val_loss: 0.3482 - val_accuracy: 0.8800\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1080/1080 [==============================] - 31s 29ms/step - loss: 0.2637 - accuracy: 0.8926 - val_loss: 0.3133 - val_accuracy: 0.8550\n",
      "Epoch 5/5\n",
      "1080/1080 [==============================] - 41s 38ms/step - loss: 0.1938 - accuracy: 0.9324 - val_loss: 0.2626 - val_accuracy: 0.8850\n",
      "Validation Confusion Matrix\n",
      "[[88 12]\n",
      " [11 89]]\n",
      "Test Confusion Matrix\n",
      "[[42 18]\n",
      " [15 45]]\n",
      "=========================================\n",
      "\n",
      "Fold 10\n",
      "====================================\n",
      "\n",
      "Train on 1080 samples, validate on 200 samples\n",
      "Epoch 1/5\n",
      "1080/1080 [==============================] - 53s 49ms/step - loss: 0.9352 - accuracy: 0.5657 - val_loss: 0.6759 - val_accuracy: 0.5500\n",
      "Epoch 2/5\n",
      "1080/1080 [==============================] - 33s 31ms/step - loss: 0.5864 - accuracy: 0.6778 - val_loss: 0.6166 - val_accuracy: 0.6550\n",
      "Epoch 3/5\n",
      "1080/1080 [==============================] - 42s 39ms/step - loss: 0.4422 - accuracy: 0.7907 - val_loss: 0.5587 - val_accuracy: 0.6900\n",
      "Epoch 4/5\n",
      "1080/1080 [==============================] - 42s 38ms/step - loss: 0.3654 - accuracy: 0.8176 - val_loss: 0.5044 - val_accuracy: 0.7250\n",
      "Epoch 5/5\n",
      "1080/1080 [==============================] - 44s 40ms/step - loss: 0.2797 - accuracy: 0.8759 - val_loss: 0.5219 - val_accuracy: 0.7450\n",
      "Validation Confusion Matrix\n",
      "[[64 36]\n",
      " [15 85]]\n",
      "Test Confusion Matrix\n",
      "[[28 32]\n",
      " [15 45]]\n",
      "##################################################################\n",
      "Test Results\n",
      "##################################################################\n",
      "Accuracy:  0.6849999999999998 \tPrecision:  0.6882227405427082 \tRecall:  0.6833333333333333 \tROC:  0.685\n",
      "Actual Positive:  600 \tPredictedPositive(TP):  412 \tFP:  188\n",
      "Actual Negative:  600 \tPredictedNegative(TN):  410 \tFN:  190\n",
      "F-measure: 0.6857693219086777\n"
     ]
    }
   ],
   "source": [
    "size = 300\n",
    "counter = 0\n",
    "best_test_accuracy = []\n",
    "after_best_test_accuracy = []\n",
    "df_pos = df_pos[0:600]\n",
    "df_neg = df_neg[0:600]\n",
    "predictedPositive = []\n",
    "predictedNegative = []\n",
    "accuracy = []\n",
    "precision= []\n",
    "recall = []\n",
    "roc = [] \n",
    "predictedPositive_V = []\n",
    "predictedNegative_V = []\n",
    "accuracy_V = []\n",
    "precision_V= []\n",
    "recall_V = []\n",
    "roc_V = [] \n",
    "for train_index, test_index in kf.split(df_pos):\n",
    "    counter = counter +1\n",
    "    print(\"=========================================\\n\")\n",
    "    print(\"Fold \"+str(counter)+\"\\n====================================\\n\")\n",
    "\n",
    "    x_train,y_train,x_test,y_test = get_train_test(df_pos,df_neg,train_index,test_index)\n",
    "    \n",
    "    x_validation = validation_df['text']\n",
    "    y_validation = validation_df['target']\n",
    "      \n",
    "    tokenizer = Tokenizer(num_words=100000)\n",
    "    tokenizer.fit_on_texts(x_train)\n",
    "    sequences = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "    length = []\n",
    "    for x in x_train:\n",
    "        length.append(len(x.split()))\n",
    "    ml = max(length)+10\n",
    "    \n",
    "    x_train_seq = pad_sequences(sequences, maxlen=ml)\n",
    "    \n",
    "    sequences_val = tokenizer.texts_to_sequences(x_validation)\n",
    "    x_val_seq = pad_sequences(sequences_val, maxlen=ml)\n",
    "    \n",
    "    sequences_test = tokenizer.texts_to_sequences(x_test)\n",
    "    x_test_seq = pad_sequences(sequences_test, maxlen=ml)\n",
    "    \n",
    "    num_words = 100000\n",
    "    embedding_matrix = np.zeros((num_words, size))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if i >= num_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    \n",
    "    tweet_input = Input(shape=(ml,), dtype='int32')\n",
    "    tweet_encoder = Embedding(100000, size, weights=[embedding_matrix], input_length=ml, trainable=True)(tweet_input)\n",
    "    unigram_branch = Conv1D(filters=32, kernel_size=1, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "    unigram_branch = GlobalMaxPooling1D()(unigram_branch)\n",
    "    bigram_branch = Conv1D(filters=32, kernel_size=2, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "    bigram_branch = GlobalMaxPooling1D()(bigram_branch)\n",
    "    trigram_branch = Conv1D(filters=32, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "    trigram_branch = GlobalMaxPooling1D()(trigram_branch)\n",
    "    fourgram_branch = Conv1D(filters=32, kernel_size=4, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "    fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
    "    \n",
    "    #merged = concatenate([unigram_branch,bigram_branch], axis=1)\n",
    "    #merged = concatenate([unigram_branch,bigram_branch, trigram_branch], axis=1)\n",
    "    merged = concatenate([unigram_branch,bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
    "    \n",
    "\n",
    "    #merged = Dense(128, activation='relu')(unigram_branch)\n",
    "    merged = Dense(32, activation='relu')(merged)\n",
    "    merged = Dropout(0.5)(merged)\n",
    "    merged = Dense(1)(merged)\n",
    "    output = Activation('sigmoid')(merged)\n",
    "    model = Model(inputs=[tweet_input], outputs=[output])\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    filepath=\"CNN_best_weights.{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "    \n",
    "    #accuracy = []\n",
    "    #class TestCallback(Callback):\n",
    "        #def on_epoch_end(self, epoch, logs={}):\n",
    "            #accuracy.append(logs.get('val_acc'))\n",
    "    test_accuracy = []\n",
    "    test_loss = []\n",
    "    class TestCallback(Callback):\n",
    "        def __init__(self, test_data):\n",
    "            self.test_data = test_data\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "            x, y = self.test_data\n",
    "            loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "            #print('\\nacc: {}\\n'.format(acc))\n",
    "            test_accuracy.append(acc)\n",
    "            test_loss.append(loss)\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    model.fit(x_train_seq, y_train, batch_size=32, epochs=5,\n",
    "                         validation_data=(x_val_seq, y_validation), callbacks=[TestCallback((x_test_seq, y_test))])\n",
    "    loss, acc = model.evaluate(x_test_seq, y_test, verbose=0)\n",
    "    ############################################################################\n",
    "    predictions = (np.asarray(model.predict(x_val_seq))).round()\n",
    "    roc_V.append(roc_auc_score(y_validation, predictions))\n",
    "    accuracy_V.append(accuracy_score(y_validation, predictions, normalize=True))\n",
    "    precision_V.append(precision_score(y_validation, predictions))\n",
    "    recall_V.append(recall_score(y_validation, predictions))\n",
    "    result = pd.DataFrame.from_items([(\"Actual\",y_validation),(\"Prediction\",predictions.tolist())])\n",
    "    print(\"Validation Confusion Matrix\")\n",
    "    cm = confusion_matrix(y_validation, predictions)\n",
    "    print(cm)\n",
    "    pp = [result['Actual'][x] for x in range(0,len(result)) if ((result['Actual'][x] ==result['Prediction'][x]) and (result['Actual'][x]==0))]\n",
    "    nn = [result['Actual'][x] for x in range(0,len(result)) if ((result['Actual'][x] ==result['Prediction'][x]) and (result['Actual'][x]==1))]\n",
    "    predictedPositive_V.append(len(pp))\n",
    "    predictedNegative_V.append(len(nn))\n",
    "    ##############################################################################\n",
    "    ############################################################################\n",
    "    predictions = (np.asarray(model.predict(x_test_seq))).round()\n",
    "    roc.append(roc_auc_score(y_test, predictions))\n",
    "    accuracy.append(accuracy_score(y_test, predictions, normalize=True))\n",
    "    precision.append(precision_score(y_test, predictions))\n",
    "    recall.append(recall_score(y_test, predictions))\n",
    "    result = pd.DataFrame.from_items([(\"Actual\",y_test),(\"Prediction\",predictions.tolist())])\n",
    "    print(\"Test Confusion Matrix\")\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    print(cm)\n",
    "    pp = [result['Actual'][x] for x in range(0,len(result)) if ((result['Actual'][x] ==result['Prediction'][x]) and (result['Actual'][x]==0))]\n",
    "    nn = [result['Actual'][x] for x in range(0,len(result)) if ((result['Actual'][x] ==result['Prediction'][x]) and (result['Actual'][x]==1))]\n",
    "    predictedPositive.append(len(pp))\n",
    "    predictedNegative.append(len(nn))\n",
    "    ##############################################################################\n",
    "    #print('\\nAfter 5 epoch Testing loss: {}, acc: {}\\n'.format(loss, acc))\n",
    "    after_best_test_accuracy.append(acc)\n",
    "    best_test_accuracy.append(max(test_accuracy))    \n",
    "    \n",
    "\n",
    "#######################################################################\n",
    "'''print(\"##################################################################\")\n",
    "print(\"Validation Results\")\n",
    "print(\"##################################################################\")\n",
    "a = sum(accuracy_V)/10\n",
    "p = sum(precision_V)/10\n",
    "r = sum(recall_V)/10\n",
    "ro = sum(roc_V)/10\n",
    "print(\"Accuracy: \",a,\"\\tPrecision: \",p,\"\\tRecall: \",r,\"\\tROC: \",ro)\n",
    "TP = sum(predictedPositive_V)/10\n",
    "FP = 100-TP\n",
    "TN = sum(predictedNegative_V)/10\n",
    "FN = 100-TN\n",
    "print(\"Actual Positive: \",len(df_pos),\"\\tPredictedPositive(TP): \",TP,\"\\tFP: \",FP)\n",
    "print(\"Actual Negative: \",len(df_neg),\"\\tPredictedNegative(TN): \",TN,\"\\tFN: \",FN)\n",
    "F_measure = (2 * p * r)/(p + r)\n",
    "print(\"F-measure: \"+str(F_measure))'''\n",
    "##############################################################################\n",
    "#######################################################################\n",
    "print(\"##################################################################\")\n",
    "print(\"Test Results\")\n",
    "print(\"##################################################################\")\n",
    "a = sum(accuracy)/10\n",
    "p = sum(precision)/10\n",
    "r = sum(recall)/10\n",
    "ro = sum(roc)/10\n",
    "print(\"Accuracy: \",a,\"\\tPrecision: \",p,\"\\tRecall: \",r,\"\\tROC: \",ro)\n",
    "TP = sum(predictedPositive)\n",
    "FP = (len(df_pos)-sum(predictedPositive))\n",
    "TN = sum(predictedNegative)\n",
    "FN = (len(df_neg)-sum(predictedNegative))\n",
    "print(\"Actual Positive: \",len(df_pos),\"\\tPredictedPositive(TP): \",TP,\"\\tFP: \",FP)\n",
    "print(\"Actual Negative: \",len(df_neg),\"\\tPredictedNegative(TN): \",TN,\"\\tFN: \",FN)\n",
    "F_measure = (2 * p * r)/(p + r)\n",
    "print(\"F-measure: \"+str(F_measure))\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result: CNN+Trained Word2vec Model Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################################################################\n",
    "Test Results\n",
    "##################################################################\n",
    "Accuracy:  0.6849999999999998 \tPrecision:  0.6882227405427082 \tRecall:  0.6833333333333333 \tROC:  0.685\n",
    "Actual Positive:  600 \tPredictedPositive(TP):  412 \tFP:  188\n",
    "Actual Negative:  600 \tPredictedNegative(TN):  410 \tFN:  190\n",
    "F-measure: 0.6857693219086777"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS: CNN+PRE-TRAINED Word2vec Model Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################################################################\n",
    "Test Results\n",
    "##################################################################\n",
    "Accuracy:  0.695 \tPrecision:  0.7030155422710656 \tRecall:  0.6900000000000001 \tROC:  0.695\n",
    "Actual Positive:  600 \tPredictedPositive(TP):  420 \tFP:  180\n",
    "Actual Negative:  600 \tPredictedNegative(TN):  414 \tFN:  186\n",
    "F-measure: 0.6964469662358497"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS IN Details: CNN+PRE-TRAINED Word2vec Model Results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========================================\n",
    "\n",
    "Fold 1\n",
    "====================================\n",
    "\n",
    "Train on 1080 samples, validate on 200 samples\n",
    "Epoch 1/5\n",
    "1080/1080 [==============================] - 20s 19ms/step - loss: 0.6919 - accuracy: 0.5361 - val_loss: 0.6371 - val_accuracy: 0.8900\n",
    "Epoch 2/5\n",
    "1080/1080 [==============================] - 17s 16ms/step - loss: 0.6142 - accuracy: 0.7417 - val_loss: 0.4937 - val_accuracy: 0.9550\n",
    "Epoch 3/5\n",
    "1080/1080 [==============================] - 17s 16ms/step - loss: 0.4326 - accuracy: 0.8796 - val_loss: 0.2157 - val_accuracy: 0.9850\n",
    "Epoch 4/5\n",
    "1080/1080 [==============================] - 17s 16ms/step - loss: 0.1762 - accuracy: 0.9657 - val_loss: 0.0618 - val_accuracy: 0.9900\n",
    "Epoch 5/5\n",
    "1080/1080 [==============================] - 17s 16ms/step - loss: 0.0579 - accuracy: 0.9954 - val_loss: 0.0200 - val_accuracy: 0.9950\n",
    "Validation Confusion Matrix\n",
    "[[ 99   1]\n",
    " [  0 100]]\n",
    "Test Confusion Matrix\n",
    "[[43 17]\n",
    " [19 41]]\n",
    "=========================================\n",
    "\n",
    "Fold 2\n",
    "====================================\n",
    "\n",
    "Train on 1080 samples, validate on 200 samples\n",
    "Epoch 1/5\n",
    "1080/1080 [==============================] - 18s 17ms/step - loss: 0.6843 - accuracy: 0.5741 - val_loss: 0.6057 - val_accuracy: 0.8400\n",
    "Epoch 2/5\n",
    "1080/1080 [==============================] - 18s 16ms/step - loss: 0.5770 - accuracy: 0.7546 - val_loss: 0.4096 - val_accuracy: 0.9400\n",
    "Epoch 3/5\n",
    "1080/1080 [==============================] - 17s 16ms/step - loss: 0.3548 - accuracy: 0.9185 - val_loss: 0.1595 - val_accuracy: 0.9900\n",
    "Epoch 4/5\n",
    "1080/1080 [==============================] - 19s 17ms/step - loss: 0.1277 - accuracy: 0.9861 - val_loss: 0.0474 - val_accuracy: 0.9950\n",
    "Epoch 5/5\n",
    "1080/1080 [==============================] - 17s 16ms/step - loss: 0.0410 - accuracy: 0.9981 - val_loss: 0.0224 - val_accuracy: 0.9950\n",
    "Validation Confusion Matrix\n",
    "[[ 99   1]\n",
    " [  0 100]]\n",
    "Test Confusion Matrix\n",
    "[[37 23]\n",
    " [20 40]]\n",
    "=========================================\n",
    "\n",
    "Fold 3\n",
    "====================================\n",
    "\n",
    "Train on 1080 samples, validate on 200 samples\n",
    "Epoch 1/5\n",
    "1080/1080 [==============================] - 18s 17ms/step - loss: 0.6886 - accuracy: 0.5407 - val_loss: 0.6089 - val_accuracy: 0.8350\n",
    "Epoch 2/5\n",
    "1080/1080 [==============================] - 17s 16ms/step - loss: 0.5783 - accuracy: 0.7759 - val_loss: 0.4281 - val_accuracy: 0.9550\n",
    "Epoch 3/5\n",
    "1080/1080 [==============================] - 18s 17ms/step - loss: 0.3754 - accuracy: 0.9111 - val_loss: 0.1797 - val_accuracy: 0.9850\n",
    "Epoch 4/5\n",
    "1080/1080 [==============================] - 17s 16ms/step - loss: 0.1433 - accuracy: 0.9843 - val_loss: 0.0488 - val_accuracy: 0.9950\n",
    "Epoch 5/5\n",
    "1080/1080 [==============================] - 17s 16ms/step - loss: 0.0486 - accuracy: 0.9963 - val_loss: 0.0193 - val_accuracy: 0.9950\n",
    "Validation Confusion Matrix\n",
    "[[ 99   1]\n",
    " [  0 100]]\n",
    "Test Confusion Matrix\n",
    "[[45 15]\n",
    " [ 9 51]]\n",
    "=========================================\n",
    "\n",
    "Fold 4\n",
    "====================================\n",
    "\n",
    "Train on 1080 samples, validate on 200 samples\n",
    "Epoch 1/5\n",
    "1080/1080 [==============================] - 18s 17ms/step - loss: 0.6879 - accuracy: 0.5435 - val_loss: 0.6204 - val_accuracy: 0.9050\n",
    "Epoch 2/5\n",
    "1080/1080 [==============================] - 17s 16ms/step - loss: 0.5907 - accuracy: 0.7954 - val_loss: 0.4447 - val_accuracy: 0.9650\n",
    "Epoch 3/5\n",
    "1080/1080 [==============================] - 17s 16ms/step - loss: 0.3873 - accuracy: 0.9028 - val_loss: 0.1845 - val_accuracy: 0.9900\n",
    "Epoch 4/5\n",
    "1080/1080 [==============================] - 17s 16ms/step - loss: 0.1424 - accuracy: 0.9796 - val_loss: 0.0422 - val_accuracy: 0.9900\n",
    "Epoch 5/5\n",
    "1080/1080 [==============================] - 17s 16ms/step - loss: 0.0416 - accuracy: 0.9954 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
    "Validation Confusion Matrix\n",
    "[[100   0]\n",
    " [  0 100]]\n",
    "Test Confusion Matrix\n",
    "[[43 17]\n",
    " [22 38]]\n",
    "=========================================\n",
    "\n",
    "Fold 5\n",
    "====================================\n",
    "\n",
    "Train on 1080 samples, validate on 200 samples\n",
    "Epoch 1/5\n",
    "1080/1080 [==============================] - 18s 17ms/step - loss: 0.6881 - accuracy: 0.5315 - val_loss: 0.6157 - val_accuracy: 0.8100\n",
    "Epoch 2/5\n",
    "1080/1080 [==============================] - 17s 16ms/step - loss: 0.5989 - accuracy: 0.7426 - val_loss: 0.4580 - val_accuracy: 0.9450\n",
    "Epoch 3/5\n",
    "1080/1080 [==============================] - 17s 16ms/step - loss: 0.4012 - accuracy: 0.8880 - val_loss: 0.1804 - val_accuracy: 0.9900\n",
    "Epoch 4/5\n",
    "1080/1080 [==============================] - 18s 16ms/step - loss: 0.1505 - accuracy: 0.9769 - val_loss: 0.0535 - val_accuracy: 0.9950\n",
    "Epoch 5/5\n",
    "1080/1080 [==============================] - 19s 17ms/step - loss: 0.0479 - accuracy: 0.9954 - val_loss: 0.0106 - val_accuracy: 1.0000\n",
    "Validation Confusion Matrix\n",
    "[[100   0]\n",
    " [  0 100]]\n",
    "Test Confusion Matrix\n",
    "[[42 18]\n",
    " [23 37]]\n",
    "=========================================\n",
    "\n",
    "Fold 6\n",
    "====================================\n",
    "\n",
    "Train on 1080 samples, validate on 200 samples\n",
    "Epoch 1/5\n",
    "1080/1080 [==============================] - 18s 17ms/step - loss: 0.6921 - accuracy: 0.5333 - val_loss: 0.6283 - val_accuracy: 0.8600\n",
    "Epoch 2/5\n",
    "1080/1080 [==============================] - 18s 16ms/step - loss: 0.5987 - accuracy: 0.7815 - val_loss: 0.4548 - val_accuracy: 0.9500\n",
    "Epoch 3/5\n",
    "1080/1080 [==============================] - 17s 16ms/step - loss: 0.3883 - accuracy: 0.9074 - val_loss: 0.1870 - val_accuracy: 0.9950\n",
    "Epoch 4/5\n",
    "1080/1080 [==============================] - 18s 16ms/step - loss: 0.1455 - accuracy: 0.9833 - val_loss: 0.0505 - val_accuracy: 0.9950\n",
    "Epoch 5/5\n",
    "1080/1080 [==============================] - 17s 16ms/step - loss: 0.0420 - accuracy: 0.9954 - val_loss: 0.0354 - val_accuracy: 0.9900\n",
    "Validation Confusion Matrix\n",
    "[[ 98   2]\n",
    " [  0 100]]\n",
    "Test Confusion Matrix\n",
    "[[50 10]\n",
    " [24 36]]\n",
    "=========================================\n",
    "\n",
    "Fold 7\n",
    "====================================\n",
    "\n",
    "Train on 1080 samples, validate on 200 samples\n",
    "Epoch 1/5\n",
    "1080/1080 [==============================] - 18s 16ms/step - loss: 0.6790 - accuracy: 0.5694 - val_loss: 0.5826 - val_accuracy: 0.9100\n",
    "Epoch 2/5\n",
    "1080/1080 [==============================] - 18s 16ms/step - loss: 0.5494 - accuracy: 0.7944 - val_loss: 0.3767 - val_accuracy: 0.9700\n",
    "Epoch 3/5\n",
    "1080/1080 [==============================] - 18s 16ms/step - loss: 0.3431 - accuracy: 0.9139 - val_loss: 0.1421 - val_accuracy: 0.9850\n",
    "Epoch 4/5\n",
    "1080/1080 [==============================] - 18s 17ms/step - loss: 0.1272 - accuracy: 0.9898 - val_loss: 0.0462 - val_accuracy: 0.9900\n",
    "Epoch 5/5\n",
    "1080/1080 [==============================] - 18s 16ms/step - loss: 0.0525 - accuracy: 0.9935 - val_loss: 0.0316 - val_accuracy: 0.9950\n",
    "Validation Confusion Matrix\n",
    "[[ 99   1]\n",
    " [  0 100]]\n",
    "Test Confusion Matrix\n",
    "[[47 13]\n",
    " [19 41]]\n",
    "=========================================\n",
    "\n",
    "Fold 8\n",
    "====================================\n",
    "\n",
    "Train on 1080 samples, validate on 200 samples\n",
    "Epoch 1/5\n",
    "1080/1080 [==============================] - 19s 17ms/step - loss: 0.6920 - accuracy: 0.5037 - val_loss: 0.6353 - val_accuracy: 0.8300\n",
    "Epoch 2/5\n",
    "1080/1080 [==============================] - 20s 19ms/step - loss: 0.6093 - accuracy: 0.7463 - val_loss: 0.4751 - val_accuracy: 0.9300\n",
    "Epoch 3/5\n",
    "1080/1080 [==============================] - 18s 17ms/step - loss: 0.4299 - accuracy: 0.8880 - val_loss: 0.2104 - val_accuracy: 0.9900\n",
    "Epoch 4/5\n",
    "1080/1080 [==============================] - 18s 16ms/step - loss: 0.1680 - accuracy: 0.9731 - val_loss: 0.0365 - val_accuracy: 1.0000\n",
    "Epoch 5/5\n",
    "1080/1080 [==============================] - 17s 16ms/step - loss: 0.0441 - accuracy: 0.9972 - val_loss: 0.0109 - val_accuracy: 1.0000\n",
    "Validation Confusion Matrix\n",
    "[[100   0]\n",
    " [  0 100]]\n",
    "Test Confusion Matrix\n",
    "[[48 12]\n",
    " [19 41]]\n",
    "=========================================\n",
    "\n",
    "Fold 9\n",
    "====================================\n",
    "\n",
    "Train on 1080 samples, validate on 200 samples\n",
    "Epoch 1/5\n",
    "1080/1080 [==============================] - 19s 17ms/step - loss: 0.6924 - accuracy: 0.5222 - val_loss: 0.6380 - val_accuracy: 0.7900\n",
    "Epoch 2/5\n",
    "1080/1080 [==============================] - 18s 17ms/step - loss: 0.5890 - accuracy: 0.7593 - val_loss: 0.5138 - val_accuracy: 0.8500\n",
    "Epoch 3/5\n",
    "1080/1080 [==============================] - 18s 16ms/step - loss: 0.3870 - accuracy: 0.9000 - val_loss: 0.3687 - val_accuracy: 0.8600\n",
    "Epoch 4/5\n",
    "1080/1080 [==============================] - 17s 16ms/step - loss: 0.1710 - accuracy: 0.9713 - val_loss: 0.3437 - val_accuracy: 0.8500\n",
    "Epoch 5/5\n",
    "1080/1080 [==============================] - 19s 18ms/step - loss: 0.0521 - accuracy: 0.9963 - val_loss: 0.4007 - val_accuracy: 0.8500\n",
    "Validation Confusion Matrix\n",
    "[[79 21]\n",
    " [ 9 91]]\n",
    "Test Confusion Matrix\n",
    "[[31 29]\n",
    " [14 46]]\n",
    "=========================================\n",
    "\n",
    "Fold 10\n",
    "====================================\n",
    "\n",
    "Train on 1080 samples, validate on 200 samples\n",
    "Epoch 1/5\n",
    "1080/1080 [==============================] - 19s 18ms/step - loss: 0.6858 - accuracy: 0.5676 - val_loss: 0.6499 - val_accuracy: 0.6800\n",
    "Epoch 2/5\n",
    "1080/1080 [==============================] - 19s 17ms/step - loss: 0.5774 - accuracy: 0.7528 - val_loss: 0.5389 - val_accuracy: 0.7800\n",
    "Epoch 3/5\n",
    "1080/1080 [==============================] - 19s 17ms/step - loss: 0.3401 - accuracy: 0.9213 - val_loss: 0.4443 - val_accuracy: 0.7900\n",
    "Epoch 4/5\n",
    "1080/1080 [==============================] - 18s 17ms/step - loss: 0.1122 - accuracy: 0.9870 - val_loss: 0.4896 - val_accuracy: 0.7800\n",
    "Epoch 5/5\n",
    "1080/1080 [==============================] - 18s 16ms/step - loss: 0.0290 - accuracy: 0.9991 - val_loss: 0.5751 - val_accuracy: 0.7850\n",
    "Validation Confusion Matrix\n",
    "[[74 26]\n",
    " [17 83]]\n",
    "Test Confusion Matrix\n",
    "[[34 26]\n",
    " [17 43]]\n",
    "##################################################################\n",
    "Test Results\n",
    "##################################################################\n",
    "Accuracy:  0.695 \tPrecision:  0.7030155422710656 \tRecall:  0.6900000000000001 \tROC:  0.695\n",
    "Actual Positive:  600 \tPredictedPositive(TP):  420 \tFP:  180\n",
    "Actual Negative:  600 \tPredictedNegative(TN):  414 \tFN:  186\n",
    "F-measure: 0.6964469662358497"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
